{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve runtime of HKY\n",
    "\n",
    "Mamie Wang 20200616\n",
    "\n",
    "We will try to improve the runtime of HKY similarity function by using the hamming distance function to compute the total mutation and subtract the other two transition rate to compute the transversion rate. \n",
    "\n",
    "We will compare the runtime using simulated sequences from a binary tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.join(os.path.dirname(sys.path[0]),'../spectral-tree-inference/spectraltree'))\n",
    "\n",
    "import numpy as np\n",
    "import utils\n",
    "import generation\n",
    "import reconstruct_tree\n",
    "import dendropy\n",
    "import scipy\n",
    "import time\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dendropy.model.discrete import simulate_discrete_chars, Jc69, Hky85\n",
    "from dendropy.calculate.treecompare import symmetric_difference\n",
    "import character_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_dist_missing_values(vals, missing_val = \"-\"):\n",
    "    classnames, indices = np.unique(vals, return_inverse=True)\n",
    "    num_arr = indices.reshape(vals.shape)\n",
    "    hamming_matrix = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(num_arr, metric='hamming'))\n",
    "    missing_array = (vals==missing_val)\n",
    "    pdist_xor = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(missing_array, lambda u,v: np.sum(np.logical_xor(u,v))))\n",
    "    pdist_or = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(missing_array, lambda u,v: np.sum(np.logical_or(u,v))))\n",
    "    \n",
    "    return (hamming_matrix*vals.shape[1] - pdist_xor) / (np.ones_like(hamming_matrix) * vals.shape[1] - pdist_or)\n",
    "\n",
    "\n",
    "def HKY_similarity_matrix_missing_data(observations, classes=None, verbose = False):\n",
    "    m, N = observations.shape\n",
    "    if classes is None:\n",
    "        classes = np.unique(observations)\n",
    "    k = len(classes)\n",
    "    # From Tamura, K., and M. Nei. 1993\n",
    "    # for each pair of sequences, \n",
    "    # 1. estimate the average base frequency for pairs of sequences\n",
    "    # 2. compute purine transition proportion P1 (A <-> G)\n",
    "    # 3. compute pyrimidine transition proportion P2 (T <-> C)\n",
    "    # 3. compute transversion proportion Q (A <-> C, A <-> T, G <-> C, G <-> T)\n",
    "\n",
    "    if verbose: print(\"Computing the average base frequency for each pair of sequences...\")\n",
    "    g = {}\n",
    "    \n",
    "    not_missing = observations != \"-\"\n",
    "    not_missing_sum = np.sum(not_missing, axis = 1) # # not missing for each taxon\n",
    "    not_missing_pair = np.array([a + b for a, b in product(not_missing_sum, repeat = 2)]).reshape((m, m))\n",
    "    \n",
    "    for x in classes:\n",
    "        obs_x = observations == x \n",
    "        g[x] = np.array([np.sum(np.hstack([a, b])) for a, b in product(obs_x, repeat = 2)]).reshape((m, m))\n",
    "        g[x] = g[x] / not_missing_pair\n",
    "\n",
    "    \n",
    "    g[\"R\"] = g[\"A\"] + g[\"G\"]\n",
    "    g[\"Y\"] = g[\"T\"] + g[\"C\"]\n",
    "    \n",
    "    # compute transition and transversion proportion\n",
    "    if verbose: print(\"Computing transition and transversion proportion for each pair of sequences...\")\n",
    "        \n",
    "    P_1 = np.zeros((m,m))\n",
    "    P_2 = np.zeros((m,m))\n",
    "    \n",
    "    A = hamming_dist_missing_values(observations, missing_val = \"-\")\n",
    "    \n",
    "    for i in range(m):\n",
    "        for j in range(i + 1, m):\n",
    "            neither_missing = np.logical_and(not_missing[i,:], not_missing[j,:])\n",
    "            a = observations[i,:][neither_missing]\n",
    "            b = observations[j,:][neither_missing]\n",
    "            \n",
    "            A_G = np.mean(np.logical_and(a == \"A\", b == \"G\") + np.logical_and(a == \"G\", b == \"A\"))\n",
    "            P_1[i, j] = P_1[j, i] = A_G\n",
    "            \n",
    "            C_T = np.mean(np.logical_and(a == \"C\", b == \"T\") + np.logical_and(a == \"T\", b == \"C\"))\n",
    "            P_2[i, j] = P_2[j, i] = C_T\n",
    "            \n",
    "    Q = A - P_1 - P_2\n",
    "    print(\"P\", P_1, P_2)\n",
    "    #print(\"Q\", Q)\n",
    "    # compute the similarity (formula 7)\n",
    "    if verbose: print(\"Computing similarity matrix\")\n",
    "    R = (1 - g[\"R\"]/(2 * g[\"A\"] * g[\"G\"]) * P_1 - 1 / (2 * g[\"R\"]) * Q)\n",
    "    Y = (1 - g[\"Y\"]/(2 * g[\"T\"] * g[\"C\"]) * P_2 - 1 / (2 * g[\"Y\"]) * Q)\n",
    "    T = (1 - 1/(2 * g[\"R\"] * g[\"Y\"]) * Q)\n",
    "    S = np.sign(R) * (np.abs(R))**(8 * g[\"A\"] * g[\"G\"] / g[\"R\"])\n",
    "    S *= np.sign(Y) * (np.abs(Y))**(8 * g[\"T\"] * g[\"C\"] / g[\"Y\"])\n",
    "    S *= np.sign(T) * (np.abs(T))**(8 * (g[\"R\"] * g[\"Y\"] - g[\"A\"] * g[\"G\"] * g[\"Y\"] / g[\"R\"] - g[\"T\"] * g[\"C\"] * g[\"R\"] / g[\"Y\"]))\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HKY_similarity_matrix_missing_data2(observations, classes=None, verbose = False):\n",
    "    m, N = observations.shape\n",
    "    if classes is None:\n",
    "        classes = np.unique(observations)\n",
    "    k = len(classes)\n",
    "    # From Tamura, K., and M. Nei. 1993\n",
    "    # for each pair of sequences, \n",
    "    # 1. estimate the average base frequency for pairs of sequences\n",
    "    # 2. compute purine transition proportion P1 (A <-> G)\n",
    "    # 3. compute pyrimidine transition proportion P2 (T <-> C)\n",
    "    # 3. compute transversion proportion Q (A <-> C, A <-> T, G <-> C, G <-> T)\n",
    "\n",
    "    if verbose: print(\"Computing the average base frequency for each pair of sequences...\")\n",
    "    g = {}\n",
    "    \n",
    "    not_missing = observations != \"-\"\n",
    "    not_missing_sum = np.sum(not_missing, axis = 1) # # not missing for each taxon\n",
    "    not_missing_pair = np.array([a + b for a, b in product(not_missing_sum, repeat = 2)]).reshape((m, m))\n",
    "    \n",
    "    for x in classes:\n",
    "        obs_x = observations == x \n",
    "        g[x] = np.array([np.sum(np.hstack([a, b])) for a, b in product(obs_x, repeat = 2)]).reshape((m, m))\n",
    "        g[x] = g[x] / not_missing_pair\n",
    "\n",
    "    \n",
    "    g[\"R\"] = g[\"A\"] + g[\"G\"]\n",
    "    g[\"Y\"] = g[\"T\"] + g[\"C\"]\n",
    "    \n",
    "    # compute transition and transversion proportion\n",
    "    if verbose: print(\"Computing transition and transversion proportion for each pair of sequences...\")\n",
    "        \n",
    "    P_1 = np.zeros((m,m))\n",
    "    P_2 = np.zeros((m,m))\n",
    "    \n",
    "    A = hamming_dist_missing_values(observations, missing_val = \"-\")\n",
    "    \n",
    "    A_G_bool = np.full_like(observations, \"-\")\n",
    "    A_G_bool[observations == \"A\"] = \"A\"\n",
    "    A_G_bool[observations == \"G\"] = \"G\"\n",
    "    P_1 = hamming_dist_missing_values(A_G_bool, missing_val = \"-\")\n",
    "    \n",
    "    \n",
    "    C_T_bool = np.full_like(observations, \"-\")\n",
    "    C_T_bool[observations == \"C\"] = \"C\"\n",
    "    C_T_bool[observations == \"T\"] = \"T\"\n",
    "    P_2 = hamming_dist_missing_values(C_T_bool, missing_val = \"-\")\n",
    "    \n",
    "            \n",
    "    Q = A - P_1 - P_2\n",
    "    print(\"P\", P_1, P_2)\n",
    "    #print(\"Q\", Q)\n",
    "    # compute the similarity (formula 7)\n",
    "    if verbose: print(\"Computing similarity matrix\")\n",
    "    R = (1 - g[\"R\"]/(2 * g[\"A\"] * g[\"G\"]) * P_1 - 1 / (2 * g[\"R\"]) * Q)\n",
    "    Y = (1 - g[\"Y\"]/(2 * g[\"T\"] * g[\"C\"]) * P_2 - 1 / (2 * g[\"Y\"]) * Q)\n",
    "    T = (1 - 1/(2 * g[\"R\"] * g[\"Y\"]) * Q)\n",
    "    S = np.sign(R) * (np.abs(R))**(8 * g[\"A\"] * g[\"G\"] / g[\"R\"])\n",
    "    S *= np.sign(Y) * (np.abs(Y))**(8 * g[\"T\"] * g[\"C\"] / g[\"Y\"])\n",
    "    S *= np.sign(T) * (np.abs(T))**(8 * (g[\"R\"] * g[\"Y\"] - g[\"A\"] * g[\"G\"] * g[\"Y\"] / g[\"R\"] - g[\"T\"] * g[\"C\"] * g[\"R\"] / g[\"Y\"]))\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 512\n",
    "binary_tree = utils.balanced_binary(m, edge_length = 0.5)\n",
    "data_HKY = simulate_discrete_chars(1000, binary_tree, Hky85(kappa = 1), mutation_rate=0.1)\n",
    "\n",
    "ch_list = list()\n",
    "for t in data_HKY.taxon_namespace:\n",
    "    ch_list.append([x.symbol for x in data_HKY[t]])\n",
    "ch_arr = np.array(ch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.26752519607544\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "mat_old = reconstruct_tree.HKY_similarity_matrix_missing_data(ch_arr)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P [[0.    0.011 0.036 ... 0.102 0.102 0.1  ]\n",
      " [0.011 0.    0.036 ... 0.101 0.1   0.097]\n",
      " [0.036 0.036 0.    ... 0.102 0.099 0.095]\n",
      " ...\n",
      " [0.102 0.101 0.102 ... 0.    0.036 0.038]\n",
      " [0.102 0.1   0.099 ... 0.036 0.    0.016]\n",
      " [0.1   0.097 0.095 ... 0.038 0.016 0.   ]] [[0.    0.014 0.024 ... 0.083 0.086 0.079]\n",
      " [0.014 0.    0.025 ... 0.08  0.09  0.083]\n",
      " [0.024 0.025 0.    ... 0.082 0.086 0.079]\n",
      " ...\n",
      " [0.083 0.08  0.082 ... 0.    0.027 0.032]\n",
      " [0.086 0.09  0.086 ... 0.027 0.    0.017]\n",
      " [0.079 0.083 0.079 ... 0.032 0.017 0.   ]]\n",
      "25.264612197875977\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "mat_new = HKY_similarity_matrix_missing_data(ch_arr)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P [[0.         0.02226721 0.07563025 ... 0.27868852 0.28021978 0.27777778]\n",
      " [0.02226721 0.         0.07775378 ... 0.28450704 0.28328612 0.27556818]\n",
      " [0.07563025 0.07775378 0.         ... 0.28813559 0.28448276 0.27536232]\n",
      " ...\n",
      " [0.27868852 0.28450704 0.28813559 ... 0.         0.07627119 0.08119658]\n",
      " [0.28021978 0.28328612 0.28448276 ... 0.07627119 0.         0.03285421]\n",
      " [0.27777778 0.27556818 0.27536232 ... 0.08119658 0.03285421 0.        ]] [[0.         0.0324826  0.05783133 ... 0.28040541 0.28289474 0.26072607]\n",
      " [0.0324826  0.         0.05995204 ... 0.26666667 0.29220779 0.26774194]\n",
      " [0.05783133 0.05995204 0.         ... 0.27242525 0.28196721 0.25901639]\n",
      " ...\n",
      " [0.28040541 0.26666667 0.27242525 ... 0.         0.06428571 0.07637232]\n",
      " [0.28289474 0.29220779 0.28196721 ... 0.06428571 0.         0.03794643]\n",
      " [0.26072607 0.26774194 0.25901639 ... 0.07637232 0.03794643 0.        ]]\n",
      "15.250773906707764\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "mat_new2 = HKY_similarity_matrix_missing_data2(ch_arr)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(mat_new - mat_old > 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3658"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(mat_new2 - mat_old > 1e-10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
